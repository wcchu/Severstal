{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marker-to-mask converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(marker_string, nv, nh):\n",
    "    # initial mask is empty 1D array\n",
    "    mask = np.zeros(nv * nh)\n",
    "    \n",
    "    # if markers exist, add detect part according to markers\n",
    "    if len(marker_string) > 0:\n",
    "        markers = np.array(marker_string.split(' ')).reshape(-1, 2)\n",
    "        for marker in markers:\n",
    "            start = int(marker[0])\n",
    "            length = int(marker[1])\n",
    "            mask[start: start+length] = 1.0\n",
    "            \n",
    "    mask = np.reshape(mask, (nv, nh))\n",
    "\n",
    "    return mask            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract the first N image IDs and their masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV header: ['ImageId_ClassId', 'EncodedPixels']\n"
     ]
    }
   ],
   "source": [
    "N = 400 # read the top N rows from file => will get N/4 masks\n",
    "D0 = 256\n",
    "D1 = 1600\n",
    "\n",
    "train_mask_file = \"data/train.csv\"\n",
    "\n",
    "with open(train_mask_file, \"r\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    print(\"CSV header: {}\".format(header))\n",
    "    \n",
    "    masks = dict()\n",
    "    for i, row in enumerate(reader):\n",
    "        if i < N:\n",
    "            # read image id and defect type\n",
    "            img_id, defect_type = row[0].split('.jpg_')\n",
    "        \n",
    "            # process mask\n",
    "            mask_marker_string = row[1]\n",
    "            mask = make_mask(mask_marker_string, D0, D1)\n",
    "        \n",
    "            if img_id in masks:\n",
    "                # if \"masks\" already has this image's mask(s), add this mask to the right channel\n",
    "                masks[img_id][:, :, int(defect_type) - 1] = mask\n",
    "            else:\n",
    "                # if \"masks\" doesn't contain this image's info, create a 4-channel mask with zeros\n",
    "                masks[img_id] = np.zeros((D0, D1, 4))  # dict({defect_type: mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(2)\n",
    "# img_id_0 = random.choice(list(masks.keys()))\n",
    "# print(img_id_0)\n",
    "# print(masks[img_id_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = 'data/train_images/'\n",
    "train_img_suffix = '.jpg'\n",
    "\n",
    "def get_image(img_id):\n",
    "    image_path = train_img_dir + img_id + train_img_suffix\n",
    "    print(image_path)\n",
    "    # import image\n",
    "    image = mpimg.imread(image_path)\n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_images/0002cc93b.jpg\n",
      "data/train_images/00031f466.jpg\n",
      "data/train_images/000418bfc.jpg\n",
      "data/train_images/000789191.jpg\n",
      "data/train_images/0007a71bf.jpg\n",
      "data/train_images/000a4bcdd.jpg\n",
      "data/train_images/000f6bf48.jpg\n",
      "data/train_images/0014fce06.jpg\n",
      "data/train_images/001982b08.jpg\n",
      "data/train_images/001d1b355.jpg\n",
      "data/train_images/001d3d093.jpg\n",
      "data/train_images/0025bde0c.jpg\n",
      "data/train_images/002af848d.jpg\n",
      "data/train_images/002e73b3c.jpg\n",
      "data/train_images/002fc4e19.jpg\n",
      "data/train_images/0030401a5.jpg\n",
      "data/train_images/003ac9d2a.jpg\n",
      "data/train_images/0046839bd.jpg\n",
      "data/train_images/005b92582.jpg\n",
      "data/train_images/005d86c25.jpg\n",
      "data/train_images/005da33cf.jpg\n",
      "data/train_images/005dbf0e0.jpg\n",
      "data/train_images/005f02e20.jpg\n",
      "data/train_images/005f19695.jpg\n",
      "data/train_images/006a4402e.jpg\n",
      "data/train_images/0074d81d0.jpg\n",
      "data/train_images/00762aa3b.jpg\n",
      "data/train_images/007f28bba.jpg\n",
      "data/train_images/008479d08.jpg\n",
      "data/train_images/008621629.jpg\n",
      "data/train_images/0088260da.jpg\n",
      "data/train_images/00894274f.jpg\n",
      "data/train_images/008b9acf5.jpg\n",
      "data/train_images/008d0f87b.jpg\n",
      "data/train_images/008ef3d74.jpg\n",
      "data/train_images/0095cd374.jpg\n",
      "data/train_images/009b12ccc.jpg\n",
      "data/train_images/00ac8372f.jpg\n",
      "data/train_images/00af2671f.jpg\n",
      "data/train_images/00b989e78.jpg\n",
      "data/train_images/00bc01bfe.jpg\n",
      "data/train_images/00bf8497a.jpg\n",
      "data/train_images/00c24a74c.jpg\n",
      "data/train_images/00c6060db.jpg\n",
      "data/train_images/00c88fed0.jpg\n",
      "data/train_images/00cdb56a0.jpg\n",
      "data/train_images/00d639396.jpg\n",
      "data/train_images/00d7ae946.jpg\n",
      "data/train_images/00dcb19f3.jpg\n",
      "data/train_images/00ded7837.jpg\n",
      "data/train_images/00e0398ad.jpg\n",
      "data/train_images/00ec97699.jpg\n",
      "data/train_images/00f1665e6.jpg\n",
      "data/train_images/00f68d337.jpg\n",
      "data/train_images/00f6e702c.jpg\n",
      "data/train_images/00f95222c.jpg\n",
      "data/train_images/00fe04c98.jpg\n",
      "data/train_images/00fe7f023.jpg\n",
      "data/train_images/0101e2cf0.jpg\n",
      "data/train_images/01053d28f.jpg\n",
      "data/train_images/010db68d1.jpg\n",
      "data/train_images/012a4581b.jpg\n",
      "data/train_images/012a9a4c7.jpg\n",
      "data/train_images/012d29df4.jpg\n",
      "data/train_images/012f26693.jpg\n",
      "data/train_images/01338c0ea.jpg\n",
      "data/train_images/0139dd004.jpg\n",
      "data/train_images/0141c9bf3.jpg\n",
      "data/train_images/014409fcc.jpg\n",
      "data/train_images/0148e9891.jpg\n",
      "data/train_images/014ebe543.jpg\n",
      "data/train_images/01540cab1.jpg\n",
      "data/train_images/015ed7fac.jpg\n",
      "data/train_images/01661826d.jpg\n",
      "data/train_images/0167a740e.jpg\n",
      "data/train_images/016af13d0.jpg\n",
      "data/train_images/016efe618.jpg\n",
      "data/train_images/01764ee81.jpg\n",
      "data/train_images/017c828a1.jpg\n",
      "data/train_images/017d75655.jpg\n",
      "data/train_images/0181695f9.jpg\n",
      "data/train_images/018ccdfed.jpg\n",
      "data/train_images/01919944c.jpg\n",
      "data/train_images/0198ab92d.jpg\n",
      "data/train_images/019f42c2a.jpg\n",
      "data/train_images/01a1027ce.jpg\n",
      "data/train_images/01afbfa7a.jpg\n",
      "data/train_images/01b043500.jpg\n",
      "data/train_images/01b237ab8.jpg\n",
      "data/train_images/01b492dd6.jpg\n",
      "data/train_images/01b6c2dc1.jpg\n",
      "data/train_images/01c3ef286.jpg\n",
      "data/train_images/01cf446d4.jpg\n",
      "data/train_images/01cfacf80.jpg\n",
      "data/train_images/01d590c5f.jpg\n",
      "data/train_images/01d97b205.jpg\n",
      "data/train_images/01dbcd2ec.jpg\n",
      "data/train_images/01df77e59.jpg\n",
      "data/train_images/01e020dc5.jpg\n",
      "data/train_images/01e501f99.jpg\n"
     ]
    }
   ],
   "source": [
    "image_list = []\n",
    "mask_list = []\n",
    "serial_list = []\n",
    "for serial, mask in masks.items():\n",
    "    image = get_image(serial)\n",
    "    image_list = image_list + [image]\n",
    "    mask_list = mask_list + [mask]\n",
    "    serial_list = serial_list + [serial]\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((np.array(image_list), np.array(mask_list)))\n",
    "ds = ds.batch(batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(strides=2),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(strides=2)#,\n",
    "    tf.keras.layers.Conv2DTranspose(8, kernel_size=3, strides=4, padding='same', activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 64, 400, 32)\n"
     ]
    }
   ],
   "source": [
    "pred_0 = model.predict(ds)\n",
    "print(pred_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f60eee8c400>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred_0[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 256, 1600, 3), (None, 256, 1600, 4)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0910 12:34:49.272613 140058284808000 training_utils.py:1436] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [10,256,1600,4] vs. [10,256,1600]\n\t [[node metrics_8/accuracy/Equal (defined at <ipython-input-37-698c273c3e92>:1) ]] [Op:__inference_keras_scratch_graph_4552]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-698c273c3e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gohome/src/github.com/wcchu/Severstal/env/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [10,256,1600,4] vs. [10,256,1600]\n\t [[node metrics_8/accuracy/Equal (defined at <ipython-input-37-698c273c3e92>:1) ]] [Op:__inference_keras_scratch_graph_4552]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(ds, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
